{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlxHtOv9Ks7U"
      },
      "source": [
        "Run first cell and cells under Import. If data files are already saved, skip the cells in Generate.\n",
        "\n",
        "If generating data, always run the cells under the Load header in Generate, then run one of the headers\n",
        "\n",
        "If you dont want to save the files to google drive, change `DATA_FOLDER` to a local path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV5Etz0DSDfx"
      },
      "outputs": [],
      "source": [
        "# DATA_FILE = \"common10\"\n",
        "# EMBEDDING_TYPE = \"cooc_pocket_100\"\n",
        "\n",
        "DATA_FILE = \"nonAnalytic\"\n",
        "EMBEDDING_TYPE = \"mpnet_abs\"\n",
        "\n",
        "DATA_FOLDER = '/content/drive/MyDrive/debateData/'\n",
        "DATA_PATH =  DATA_FOLDER + DATA_FILE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m38pzvHSD4h"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS01WcRNR-Il",
        "outputId": "0df0b9f8-d0d8-47b6-f102-3ac9d5f9785d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if DATA_FOLDER.startswith(\"/content/drive/MyDrive/\"):\n",
        "  from google.colab import drive\n",
        "  from os import mkdir\n",
        "  from pathlib import Path\n",
        "\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  Path(DATA_FOLDER).mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lGnlM0oSZhK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from scipy import sparse\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWqFjJ5mKH9j"
      },
      "source": [
        "# Generate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load"
      ],
      "metadata": {
        "id": "03HebrXfUkyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ELUaI1dH9OL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV7Qk8HmH5Df"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "all_evidence = load_dataset(\"Yusuf5/OpenCaselistTMP\", split=\"train\").to_pandas().set_index('id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0g59i0iAnT4",
        "outputId": "5b24ef9b-1d80-45a0-8ce6-9bd6964071f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "570452 Buckets Loaded\n"
          ]
        }
      ],
      "source": [
        "if DATA_FILE == \"common10\":\n",
        "  common10_evidence = all_evidence[all_evidence.duplicateCount >= 10]\n",
        "  evidence = common10_evidence\n",
        "elif DATA_FILE == \"nonAnalytic\":\n",
        "  nonAnalytic_evidence = all_evidence.loc[all_evidence.textLength > 0]\n",
        "  evidence = nonAnalytic_evidence\n",
        "else:\n",
        "  evidence = all_evidence\n",
        "bucketCards = evidence.drop_duplicates(\"bucketId\")\n",
        "bucketCards.bucketId.to_csv(DATA_PATH + \"_bucketIds.csv\")\n",
        "print(f\"{len(bucketCards)} Buckets Loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eME2nUUzIcd"
      },
      "source": [
        "## LI Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH0HzNXSw5MZ"
      },
      "outputs": [],
      "source": [
        "evidence = all_evidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPyjxc8xwxqv"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "def checkHeaders(data: pd.DataFrame, search: str, tag=False):\n",
        "  search = search.lower()\n",
        "  matches = [data[header].str.lower().str.contains(search) for header in ['pocket', 'hat', 'block'] + (['tag'] if tag else [])]\n",
        "  return reduce(lambda a, b: a | b, matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnItCAfPv9Js"
      },
      "outputs": [],
      "source": [
        "links = checkHeaders(evidence, 'link')\n",
        "impacts = checkHeaders(evidence, 'impact')\n",
        "liInfo = pd.DataFrame({\n",
        "  'bucketId': evidence.bucketId,\n",
        "  'link': links,\n",
        "  'impact': impacts\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Btl6nn3oySL-"
      },
      "outputs": [],
      "source": [
        "liCounts = liInfo.groupby('bucketId').sum()\n",
        "duplicate = liCounts.loc[(liCounts.link != 0) & (liCounts.impact != 0)]\n",
        "duplicateRatios = np.log2(duplicate.link / duplicate.impact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afgpzaPvyT5a"
      },
      "outputs": [],
      "source": [
        "linkBuckets = liInfo[liInfo.link].bucketId.values\n",
        "impactBuckets = liInfo[liInfo.impact].bucketId.values\n",
        "\n",
        "realLinks = np.union1d(np.setdiff1d(linkBuckets, impactBuckets), duplicateRatios[duplicateRatios > 1].index)\n",
        "realImpacts = np.union1d(np.setdiff1d(impactBuckets, linkBuckets), duplicateRatios[duplicateRatios < -1].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FodbumNyUnW"
      },
      "outputs": [],
      "source": [
        "evidence['link'] = evidence.bucketId.isin(realLinks)\n",
        "evidence['impact'] = evidence.bucketId.isin(realImpacts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cg2AHdQUybWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e490c2d6-e3f5-4699-d2e1-9a3b699915c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 67503 links and 57079 impacts\n"
          ]
        }
      ],
      "source": [
        "evidence.drop_duplicates(\"bucketId\").reset_index()[['link', 'impact', 'bucketId']].to_feather(DATA_FOLDER + \"all_li.feather\")\n",
        "print(f\"Saved {len(realLinks)} links and {len(realImpacts)} impacts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0_hwAN0zPq1"
      },
      "source": [
        "## Co-occurence Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRT6yRxssNpy"
      },
      "outputs": [],
      "source": [
        "window_level = \"block\"\n",
        "dimension = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUDf9fkIOOM1"
      },
      "outputs": [],
      "source": [
        "bucketIds = bucketCards.reset_index().bucketId\n",
        "bucketIndexes = pd.Series(bucketIds.index, bucketIds.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDeUTucWsENu"
      },
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "import itertools\n",
        "\n",
        "levels = ['fileId', 'pocket', 'hat', 'block']\n",
        "def build_co_occurrence():\n",
        "  M = sparse.lil_matrix((len(bucketIds), len(bucketIds)), dtype=np.int32)\n",
        "\n",
        "  groups = evidence.groupby(levels[0:levels.index(window_level) + 1])\n",
        "  for name, group in tqdm(groups):\n",
        "    # Sparse matricies cant really vecotrize this\n",
        "    for edge in itertools.combinations(bucketIndexes[group.bucketId], 2):\n",
        "      M[edge] += 1\n",
        "  return (M + M.T).tocsr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCPLpQ3lsGDt"
      },
      "outputs": [],
      "source": [
        "# Progress bar shows more iterations than it should\n",
        "try:\n",
        "  co_occurence = sparse.load_npz(f'{DATA_PATH}_cooc_{window_level}.npz').tocsr()\n",
        "except FileNotFoundError:\n",
        "  co_occurence = build_co_occurrence()\n",
        "  sparse.save_npz(f'{DATA_PATH}_cooc_{window_level}.npz', co_occurence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv1QPNxStjVl"
      },
      "outputs": [],
      "source": [
        "def total_to_divideMatrix(totals: np.array):\n",
        "  with np.errstate(divide='ignore'):\n",
        "    M = 1 / totals\n",
        "  M[np.isinf(M)] = 0\n",
        "  return sparse.diags(M)\n",
        "\n",
        "def build_ppmi(M: sparse.csr_matrix):\n",
        "  probabilities = M.copy()\n",
        "\n",
        "  # Will be same when symmetric\n",
        "  word_totals = np.array(co_occurence.sum(axis=0))[0]\n",
        "  context_totals = np.array(co_occurence.sum(axis=1))[:, 0]\n",
        "\n",
        "  # pmi_wc = log(P(w, c) / (P(w) * P(c)))\n",
        "  #        = log((#(w, c) / total) / ((#(w) / total) * (#(c) / total)))\n",
        "  #        = log(#(w, c) * total) / (#(w) * #(c))\n",
        "\n",
        "  #(w, c) * total\n",
        "  probabilities *= word_totals.sum()\n",
        "\n",
        "  word_divider = total_to_divideMatrix(word_totals)\n",
        "  context_divider = total_to_divideMatrix(context_totals)\n",
        "  # / ((#w) * (#c))\n",
        "  probabilities = word_divider @ probabilities @ context_divider # Divide each row by word_divider, column by context_divider\n",
        "\n",
        "  probabilities.data = np.maximum(probabilities.data, 1) # Clamps log to 0 for ppmi\n",
        "  probabilities.data = np.log2(probabilities.data)\n",
        "  return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3soLwHStqqR"
      },
      "outputs": [],
      "source": [
        "ppmi = build_ppmi(co_occurence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVFWF2eGt4zc"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Takes a few minutes\n",
        "n_iters = 10\n",
        "svd = TruncatedSVD(n_components=dimension, n_iter=n_iters)\n",
        "embeddings = svd.fit_transform(ppmi)\n",
        "embeddings = normalize(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H6k4oonuacU"
      },
      "outputs": [],
      "source": [
        "np.save(f'{DATA_PATH}_embeddings_cooc_{window_level}_{dimension}', embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkF4V-ANdt9Y"
      },
      "source": [
        "## Transformer Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRNRvaTGdy50"
      },
      "source": [
        "### Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Egm88eak6fX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4NcGiZYfYYv"
      },
      "source": [
        "### Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkX4xsLsfadV"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkeBpqug8ibu"
      },
      "outputs": [],
      "source": [
        "# Time estimate will be way to long for a while\n",
        "sbertEmbeddings = model.encode(list(bucketCards.tag), show_progress_bar=True, normalize_embeddings=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ_qVbeGBmPg"
      },
      "outputs": [],
      "source": [
        "np.save(DATA_PATH + \"_embeddings_mpnet_abs.npy\", sbertEmbeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQcKq2-HAty8"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrbQAHzkqk_i"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "hidden_size = 200\n",
        "learning_rate = 1e-3\n",
        "epochs = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a79npcE217fn"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split"
      ],
      "metadata": {
        "id": "u0980yVNLAtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYY4u-YSZO99"
      },
      "outputs": [],
      "source": [
        "embeddings = np.load(f'{DATA_PATH}_embeddings_{EMBEDDING_TYPE}.npy')\n",
        "bucketIds = pd.read_csv(DATA_PATH + '_bucketIds.csv').bucketId\n",
        "bucketIndexes = pd.Series(bucketIds.index, bucketIds.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uYOwdXuZac3"
      },
      "outputs": [],
      "source": [
        "liLabels = pd.read_feather(DATA_FOLDER + \"all_li.feather\").set_index(\"bucketId\")\n",
        "liLabels = liLabels.loc[bucketIds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CKhFKjWpIrQ"
      },
      "outputs": [],
      "source": [
        "labeled = liLabels[liLabels.link | liLabels.impact].index\n",
        "labeledIndexes = pd.Series(np.arange(len(labeled)), labeled)\n",
        "inputs = torch.tensor(embeddings[bucketIndexes[labeled]], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z8u2GMspV20"
      },
      "outputs": [],
      "source": [
        "labels = torch.zeros((len(labeled)), dtype=torch.long)\n",
        "labels[labeledIndexes[liLabels.index[liLabels.link]].values] = 0\n",
        "labels[labeledIndexes[liLabels.index[liLabels.impact]].values] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhMAglqDpYZn"
      },
      "outputs": [],
      "source": [
        "data = list(zip(inputs, labels))\n",
        "train_split, validate_split, test_split = random_split(data, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(42))\n",
        "train_loader = DataLoader(train_split, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "validate_loader = DataLoader(validate_split, batch_size=1)\n",
        "inputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oHXjJGPqo_z"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wymppI-qsA9"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(Classifier, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Linear(input_size, hidden_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.75),\n",
        "      nn.Linear(hidden_size, num_classes),\n",
        "      nn.LogSoftmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W4gNz9vmyZQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l_XwfT-qvTL"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0PuZb-SqytC"
      },
      "outputs": [],
      "source": [
        "train_loss_history = []\n",
        "validate_loss_history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g64Soamwq01i"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model: nn.Module, optimizer: optim.Optimizer, epoch: int, log_freq=0):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  for i, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ouputs = model(data)\n",
        "\n",
        "    loss = F.nll_loss(ouputs, target)\n",
        "    loss.backward()\n",
        "\n",
        "    train_loss += loss.item() * len(data)\n",
        "    optimizer.step()\n",
        "\n",
        "    if log_freq and (i + 1) % log_freq == 0:\n",
        "      percentage = 100 * (i + 1) * len(data) / len(train_loader.dataset)\n",
        "      print(f'Epoch: {epoch} [{(i + 1) * len(data)}/{len(train_loader.dataset)} ({percentage:.3f}%)], Loss: {loss.item():.6f}')\n",
        "  train_loss_history.append(train_loss / len(train_loader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep3KScryq1ud"
      },
      "outputs": [],
      "source": [
        "def validate(model: nn.Module, epoch: int):\n",
        "  model.eval()\n",
        "  validate_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in validate_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      validate_loss += F.nll_loss(output, target).item()\n",
        "      prediction = output.argmax(dim=1, keepdim=True)\n",
        "      correct += prediction.eq(target).sum().item()\n",
        "\n",
        "  validate_loss /= len(validate_loader.dataset)\n",
        "  validate_loss_history.append(validate_loss)\n",
        "\n",
        "  print(f\"Validate set (Epoch {epoch}): Average Loss {validate_loss:.4f}, Accuracy: {correct}/{len(validate_loader.dataset)} ({100 * correct / len(validate_loader.dataset):.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op34f7Dvq38L"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EISpQnIuq4eg"
      },
      "outputs": [],
      "source": [
        "model = Classifier(inputs.shape[1], hidden_size, 2).to(device)\n",
        "adamW = optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vxLU-bbq5_E"
      },
      "outputs": [],
      "source": [
        "train_loss_history = []\n",
        "validate_loss_history = []\n",
        "for i in range(0, 0 + epochs):\n",
        "  train_epoch(model, adamW, i)\n",
        "  validate(model, i + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZTfaX0Qq7F1"
      },
      "outputs": [],
      "source": [
        "plt.close()\n",
        "plt.plot(train_loss_history, label='train')\n",
        "plt.plot(validate_loss_history, label='validate')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HXjz2Mot24oJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "9m38pzvHSD4h",
        "KWqFjJ5mKH9j",
        "03HebrXfUkyj",
        "4eME2nUUzIcd",
        "e0_hwAN0zPq1",
        "XkF4V-ANdt9Y",
        "a79npcE217fn",
        "0oHXjJGPqo_z",
        "_W4gNz9vmyZQ"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}